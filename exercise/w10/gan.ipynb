{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoehbwyzqUhv",
        "outputId": "3f430745-fa51-4098-f143-33bf37c72680"
      },
      "id": "WoehbwyzqUhv",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from np_utils) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "805d42e6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "805d42e6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten, LeakyReLU\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Lấy dữ liệu từ bộ MNIST\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8cb5659"
      },
      "source": [
        "\n",
        "# Tiền xử lý dữ liệu, reshape từ ảnh xám 28*28 thành vector 784 chiều và đưa dữ liệu từ scale [0, 255] về [0, 1]\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255"
      ],
      "id": "c8cb5659",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Số chiều noise vector\n",
        "z_dim = 100\n"
      ],
      "metadata": {
        "id": "kDcMQNt8poZw"
      },
      "id": "kDcMQNt8poZw",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "adam = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "# Mô hình Generator\n",
        "g = Sequential()\n",
        "g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\n",
        "g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
        "# Vì dữ liệu ảnh MNIST đã chuẩn hóa về [0, 1] nên hàm G khi sinh ảnh ra cũng cần sinh ra ảnh có pixel value trong khoảng [0, 1] => hàm sigmoid được chọn\n",
        "g.add(Dense(784, activation='sigmoid'))\n",
        "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# Mô hình Discriminator\n",
        "d = Sequential()\n",
        "d.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))\n",
        "d.add(Dropout(0.3))\n",
        "d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "d.add(Dropout(0.3))\n",
        "d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
        "d.add(Dropout(0.3))\n",
        "# Hàm sigmoid cho bài toán binary classification\n",
        "d.add(Dense(1, activation='sigmoid'))\n",
        "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "d.trainable = False\n",
        "inputs = Input(shape=(z_dim, ))\n",
        "hidden = g(inputs)\n",
        "output = d(hidden)\n",
        "gan = Model(inputs, output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTVUIIEos-zj",
        "outputId": "3f69b1b6-0e44-40eb-91a2-b1b373af28ea"
      },
      "id": "aTVUIIEos-zj",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm vẽ loss function\n",
        "def plot_loss(losses):\n",
        "    d_loss = [v[0] for v in losses[\"D\"]]\n",
        "    g_loss = [v[0] for v in losses[\"G\"]]\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Hàm vẽ sample từ Generator\n",
        "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\n",
        "    noise = np.random.normal(0, 1, size=(n_ex, z_dim))\n",
        "    generated_images = g.predict(noise)\n",
        "    generated_images = generated_images.reshape(n_ex, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gN7OqrSntFJj"
      },
      "id": "gN7OqrSntFJj",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "osses = {\"D\":[], \"G\":[]}\n",
        "\n",
        "def train(epochs=1, plt_frq=1, BATCH_SIZE=128):\n",
        "    # Tính số lần chạy trong mỗi epoch\n",
        "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\n",
        "    print('Epochs:', epochs)\n",
        "    print('Batch size:', BATCH_SIZE)\n",
        "    print('Batches per epoch:', batchCount)\n",
        "\n",
        "    for e in tqdm(range(1, epochs+1)):\n",
        "        if e == 1 or e%plt_frq == 0:\n",
        "            print('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in range(batchCount):\n",
        "            # Lấy ngẫu nhiên các ảnh từ MNIST dataset (ảnh thật)\n",
        "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]\n",
        "            # Sinh ra noise ngẫu nhiên\n",
        "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
        "\n",
        "            # Dùng Generator sinh ra ảnh từ noise\n",
        "            generated_images = g.predict(noise)\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            # Tạo label\n",
        "            y = np.zeros(2*BATCH_SIZE)\n",
        "            y[:BATCH_SIZE] = 0.9  # gán label bằng 0.9 cho những ảnh từ MNIST dataset và 0 cho ảnh sinh ra bởi Generator\n",
        "\n",
        "            # Train discriminator\n",
        "            d.trainable = True\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
        "            # Khi train Generator gán label bằng 1 cho những ảnh sinh ra bởi Generator -> cố gắng lừa Discriminator.\n",
        "            y2 = np.ones(BATCH_SIZE)\n",
        "            # Khi train Generator thì không cập nhật hệ số của Discriminator.\n",
        "            d.trainable = False\n",
        "            g_loss = gan.train_on_batch(noise, y2)\n",
        "\n",
        "        # Lưu loss function\n",
        "        losses[\"D\"].append(d_loss)\n",
        "        losses[\"G\"].append(g_loss)\n",
        "\n",
        "        # Vẽ các số được sinh ra để kiểm tra kết quả\n",
        "        if e == 1 or e%plt_frq == 0:\n",
        "            plot_generated()\n",
        "    plot_loss(losses)"
      ],
      "metadata": {
        "id": "MRqn-ByKwVKj"
      },
      "id": "MRqn-ByKwVKj",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(epochs=200, plt_frq=20, BATCH_SIZE=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "1qVZQt3btbFq",
        "outputId": "a516eb91-e7d1-45c1-e4e2-a8487d7ee14f"
      },
      "id": "1qVZQt3btbFq",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 200\n",
            "Batch size: 128\n",
            "Batches per epoch: 468\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqdm_notebook' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3353559938.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt_frq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2396784649.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, plt_frq, BATCH_SIZE)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batches per epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mplt_frq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm_notebook' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OURUGll-td-5"
      },
      "id": "OURUGll-td-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd989871"
      },
      "source": [
        "## Define loss functions and optimizers\n",
        "\n",
        "### Subtask:\n",
        "Define the loss functions and optimizers separately for the generator and discriminator.\n"
      ],
      "id": "fd989871"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24f98a5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the loss function and optimizers for the generator and discriminator as instructed.\n",
        "\n"
      ],
      "id": "d24f98a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b6fbf4b"
      },
      "source": [],
      "id": "3b6fbf4b",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bf022a8"
      },
      "source": [
        "## Implement the training step using tf.gradienttape\n",
        "\n",
        "### Subtask:\n",
        "Create a function that performs a single training step for both the generator and discriminator using `tf.GradientTape` to calculate gradients and apply them.\n"
      ],
      "id": "4bf022a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4359bbd2"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `train_step` function to perform one training iteration for both the generator and discriminator using `tf.GradientTape`.\n",
        "\n"
      ],
      "id": "4359bbd2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "280d5969"
      },
      "source": [],
      "id": "280d5969",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0bf7bd"
      },
      "source": [
        "## Update the training loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the main training loop to call the custom training step function for each batch.\n"
      ],
      "id": "1b0bf7bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9992fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to use the custom `train_step` function and remove the manual training steps for the discriminator and generator.\n",
        "\n"
      ],
      "id": "eb9992fa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ed65b5"
      },
      "source": [],
      "id": "75ed65b5",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66b4f653"
      },
      "source": [
        "## Update loss tracking and plotting\n",
        "\n",
        "### Subtask:\n",
        "Adjust the code to track and plot the losses from the custom training step.\n"
      ],
      "id": "66b4f653"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af916412"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The loss functions for the generator and discriminator were defined using `tf.keras.losses.BinaryCrossentropy`.\n",
        "*   Separate Adam optimizers were created for the generator and discriminator, both with a learning rate of 0.0002 and $\\beta_1=0.5$.\n",
        "*   A custom `train_step` function was implemented using `tf.GradientTape` to calculate and apply gradients for both models.\n",
        "*   The main training loop was updated to call the `train_step` function for each batch and track the returned generator and discriminator losses.\n",
        "*   The code for tracking and plotting the generator and discriminator losses over epochs was confirmed to be correctly implemented and utilizing the captured losses.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The implementation successfully integrates custom training logic using TensorFlow's gradient tape for a GAN.\n",
        "*   The next logical step would be to train the model for a sufficient number of epochs and evaluate the quality of the generated images to assess the training progress.\n"
      ],
      "id": "af916412"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}